---
title: "Day 1_Online Retail"
author: "Kolade Agboola"
date: "2025-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Installing necessary packages for this analysis
```{r, message  = FALSE, echo = FALSE}
library(tidyverse) ## dplyr, ggplot2 are all in here
library(lubridate)
library(janitor)
library(readxl)
library(data.table)
library(cluster)
library(factoextra)
library(scatterplot3d)
library(plotly)
library(naniar)
```


# Importing the dataset
```{r}
# Loading the excel file
file_path <- "file_path.xlsx"

# Reading the two sheets (2009-2010 and 2010-2011)
online_retail_2009_2010 <- read_excel(file_path, sheet = 1)
online_retail_2010_2011 <- read_excel(file_path, sheet = 2)

# Combining the two datasets
retail_data <- bind_rows(online_retail_2009_2010, online_retail_2010_2011)
```

# Examining Dataset
```{r}
head(retail_data, 10) # Showing top 10 in the data
tail(retail_data, 10) # showing bottom 10 in the data
str(retail_data) # sh0wing structure of the data
colnames(retail_data) # showing column names of the data
summary(retail_data) # showing summary statistics of the data
glimpse(retail_data) # giving us a glimpse of what to expect in the data
```


# Checking for missing value
```{r}
#Visualising to see if there are missing values
gg_miss_var(retail_data)

# Create a summary of missing values for each column
missing_summary <- data.frame(
  column = names(retail_data),
  missing_count = colSums(is.na(retail_data)),
  missing_percentage = colSums(is.na(retail_data)) / nrow(retail_data) * 100
)

# Print  summary
print(missing_summary)
```


# Cleaning Dataset
```{r}
nrow(retail_data) # Number of rows before cleaning

# Changing the format of the column names
clean_retail_data <- retail_data |>
  clean_names()
colnames(clean_retail_data) # showing us column name after cleaning




##### Checking for duplicates
### Round 1 check
##deduped.data <- unique( clean_retail_data[ , 1:8 ] )

###Round 2
clean_retail_data <- distinct(clean_retail_data)


# # Check for exact duplicate rows (all columns considered)
# exact_duplicates <- clean_retail_data %>%
#   filter(duplicated(.))
# 
# # Count exact duplicates
# cat("Total Rows in Combined Dataset:", nrow(clean_retail_data), "\n")
# cat("Exact Duplicate Rows (All Columns):", nrow(exact_duplicates), "\n")
# 
# # Remove exact duplicate rows (keeping the first occurrence)
# combined_cleaned <- clean_retail_data %>% distinct()
# 
# # Aggregate sales data (sum Quantity, average Price) after removing duplicates
# df_combined_aggregated <- combined_cleaned %>%
#   group_by(invoice, stock_code, description, customer_id, country) %>%
#   summarise(Quantity = sum(quantity), Price = mean(price), .groups = 'drop')


# Convert date column to Date format
clean_retail_data <- clean_retail_data |>
  mutate(invoice_date = as.POSIXct(invoice_date, format = "%Y-%m-%d %H:%M:%S"))

# Calculating the total sales for each row
clean_retail_data <- clean_retail_data |>
  mutate(total_sales = quantity * price)

# Remove rows with missing customer IDs
clean_retail_data <- clean_retail_data |>
  filter(!is.na(customer_id)) |>
  filter(!is.na(description))

# Separate canceled and valid transactions
cancelled_transactions <- clean_retail_data |>
  filter(grepl("^C", invoice))

valid_transactions_before <- clean_retail_data |>
  filter(!grepl("^C", invoice))

# Create separate columns for date and time
clean_retail_data <- clean_retail_data |>
  mutate(
    date_only = as.Date(invoice_date),           # Extract the date
    time_only = format(invoice_date, "%H:%M:%S") # Extract the time
  )


head(clean_retail_data) # View the dataset to confirm the new columns

nrow(clean_retail_data) # Number of rows of main data after cleaning

nrow(cancelled_transactions) # Number of rows of main data after cleaning

nrow(valid_transactions_before) # Number of rows of main data after cleaning

# Converting time_only to a time object
clean_retail_data <- clean_retail_data |> 
  mutate(time_only = hms(time_only))

# Check structure of the time_only column
str(clean_retail_data)
```


# Checking for Outliers
```{r}
# ----------------- Outlier Detection -----------------

# Boxplot visualization for outliers
ggplot(valid_transactions_before, aes(y = quantity)) + geom_boxplot() + ggtitle("Outliers in Quantity")
ggplot(valid_transactions_before, aes(y = price)) + geom_boxplot() + ggtitle("Outliers in Price")

# Histogram before removing outliers
hist_quantity_before <- ggplot(valid_transactions_before, aes(x = quantity)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  ggtitle("Quantity Distribution (Before Outlier Removal)") 

hist_price_before <- ggplot(valid_transactions_before, aes(x = price)) +
  geom_histogram(bins = 30, fill = "red", color = "black", alpha = 0.7) +
  ggtitle("Price Distribution (Before Outlier Removal)") 

hist_quantity_before
hist_price_before


# Function to remove outliers using IQR (Interquartile Range)
remove_outliers <- function(data, column) {
  Q1 <- quantile(data[[column]], 0.25, na.rm = TRUE)
  Q3 <- quantile(data[[column]], 0.75, na.rm = TRUE)
  IQR_value <- Q3 - Q1
  
  lower_bound <- Q1 - 1.5 * IQR_value
  upper_bound <- Q3 + 1.5 * IQR_value
  
  # Keep only values within bounds
  data <- data %>% filter(data[[column]] >= lower_bound & data[[column]] <= upper_bound)
  return(data)
}

# Remove outliers from Quantity and Price
valid_transactions_after <- valid_transactions_before |>
  remove_outliers("quantity") |>
  remove_outliers("price")



############ Outliers for further Inspection #######################

# Get all rows in valid_transactions_before that are NOT in valid_transactions_after
outliers_removed <- anti_join(valid_transactions_before, valid_transactions_after, by = colnames(valid_transactions_before))

##############################################################


# Histogram after removing outliers
hist_quantity_after <- ggplot(valid_transactions_after, aes(x = quantity)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  ggtitle("Quantity Distribution (After Outlier Removal)") 


hist_price_after <- ggplot(valid_transactions_after, aes(x = price)) +
  geom_histogram( fill = "red", color = "black", alpha = 0.7) +
  ggtitle("Price Distribution (After Outlier Removal)") 
#+  xlim(min(valid_transactions$price), max(valid_transactions$price))

hist_quantity_after
hist_price_after


ggplot(valid_transactions_after, aes(y = quantity)) + geom_boxplot() + ggtitle("After Outliers removed in Quantity")
ggplot(valid_transactions_after, aes(y = price)) + geom_boxplot() + ggtitle("After Outliers removed in Price")
```


# trying to see if there is any more missing data in the dataset
```{r}
#Visualising to see if there are missing values
gg_miss_var(valid_transactions_after)
# Create a summary of missing values for each column
missing_summary <- data.frame(
  column = names(valid_transactions_after),
  missing_count = colSums(is.na(valid_transactions_after)),
  missing_percentage = colSums(is.na(valid_transactions_after)) / nrow(valid_transactions_after) * 100
)

# Print the summary
print(missing_summary)
```



# Sales Trend Analysis
```{r}
# 1. Daily Sales Trend
daily_sales <- valid_transactions_after |>
  group_by(date = as.Date(invoice_date)) |>
  summarise(total_sales = sum(total_sales))

ggplot(daily_sales, aes(x = date, y = total_sales)) +
  geom_line(color = "blue") +
  labs(title = "Daily Sales Trend", x = "Date", y = "Total Sales")

# 2. Weekly Sales Trend
weekly_sales <- valid_transactions_after |>
  group_by(week = floor_date(invoice_date, unit = "week")) |>
  summarise(total_sales = sum(total_sales))

ggplot(weekly_sales, aes(x = week, y = total_sales)) +
  geom_line(color = "green") +
  labs(title = "Weekly Sales Trend", x = "Week", y = "Total Sales")

# 3. Monthly Sales Trend
monthly_sales <- valid_transactions_after |>
  group_by(month = floor_date(invoice_date, unit = "month")) |>
  summarise(total_sales = sum(total_sales))

ggplot(monthly_sales, aes(x = month, y = total_sales)) +
  geom_line(color = "orange") +
  labs(title = "Monthly Sales Trend", x = "Month", y = "Total Sales")

# 4. Quarterly Sales Trend
quarterly_sales <- valid_transactions_after |>
  group_by(quarter = floor_date(invoice_date, unit = "quarter")) |>
  summarise(total_sales = sum(total_sales))

ggplot(quarterly_sales, aes(x = quarter, y = total_sales)) +
  geom_line(color = "purple") +
  labs(title = "Quarterly Sales Trend", x = "Quarter", y = "Total Sales")

# 5. Yearly Sales Trend
yearly_sales <- valid_transactions_after |>
  group_by(year = year(invoice_date)) |>
  summarise(total_sales = sum(total_sales))

ggplot(yearly_sales, aes(x = as.factor(year), y = total_sales)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(title = "Yearly Sales Trend", x = "Year", y = "Total Sales")
```

# Heat map of total sales and sales count.
```{r}
# Add day_of_week column manually
valid_transactions_after <- valid_transactions_after |>
  mutate(
    hour = hour(invoice_date),
    day_of_week_num = wday(invoice_date),  # Extract numeric day of the week (1 = Sunday, 7 = Saturday)
    day_of_week = factor(day_of_week_num, levels = 1:7, 
                         labels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))
  )

# View the first few rows
head(valid_transactions_after$day_of_week)

# Extract time and day information
heatmap_data <- valid_transactions_after |>
  group_by(day_of_week, hour) |>
  summarise(total_sales = sum(total_sales), sales_count = n())

# Heatmap for Total Sales
ggplot(heatmap_data, aes(x = hour, y = day_of_week, fill = total_sales)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Heatmap of Total Sales by Time and Day", x = "Hour of Day", y = "Day of Week", fill = "Total Sales")

# Heatmap for Sales Count
ggplot(heatmap_data, aes(x = hour, y = day_of_week, fill = sales_count)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightyellow", high = "darkred") +
  labs(title = "Heatmap of Sales Count by Time and Day", x = "Hour of Day", y = "Day of Week", fill = "Sales Count")

```

# Customer Segmentation Test 1....Test 2 works better
```{r}
# # Compute RFM metrics
# rfm_data <- valid_transactions |>
#   group_by(customer_id) |>
#   summarise(
#     recency = as.numeric(difftime(max(invoice_date), Sys.Date(), units = "days")),
#     frequency = n_distinct(invoice),
#     monetary = sum(total_sales)
#   )
# 
# # Normalize RFM data
# rfm_scaled <- scale(rfm_data[, -1])
# 
# # Perform K-Means clustering
# set.seed(123)
# rfm_clusters <- kmeans(rfm_scaled, centers = 4, nstart = 10)
# 
# # Add cluster labels
# rfm_data$cluster <- as.factor(rfm_clusters$cluster)
# 
# # Visualize RFM clusters
# fviz_cluster(
#   object = rfm_clusters,    # K-Means clustering object
#   data = rfm_scaled,        # Scaled data used for clustering
#   geom = "point",           # Plot points for clusters
#   stand = FALSE             # Data is already scaled
# ) +
#   labs(title = "RFM Clusters - K-Means Clustering")

```

# Customer Segmentation Test 2
# https://www.linkedin.com/pulse/customer-insights-rfm-analysis-enhanced-k-means-clustering-sahal-naz-t87cc
```{r}
# Compute RFM metrics
rfm_data_1 <- valid_transactions_after |>
  group_by(customer_id) |>
  summarise(
    #recency_1 = as.numeric(difftime(max(invoice_date), Sys.Date(), units = "days"))
    recency_1 = as.numeric(difftime(Sys.Date(), max(invoice_date), units = "days")),
    frequency_1 = n_distinct(invoice),
    monetary_1 = sum(total_sales)
  )

# standardize RFM Values
rfm_scaled_1 <- scale(rfm_data_1[, -1])
# rfm_scaled_1 <- scale(rfm_data_1[, 2:4]) # This does the same thing as the top by selecting all columns but the first one.


# Determining the optimal number of clusters for segmentation using Elbow Method.
# Silhoutte Scores is another method to use
cluster_elbow <- (nrow(rfm_scaled_1) - 1) * sum(apply(rfm_scaled_1, 2, var))

for (i in 1:15) cluster_elbow[i] <-
  sum(kmeans(rfm_scaled_1, centers = i)$withinss)

plot(1:15, cluster_elbow, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")
### Optimal number of clusters using the elbow method for this data is 8


#Performing k-means clustering
set.seed(911)
kmeans_model <- kmeans(rfm_scaled_1, centers = 8, nstart = 25)

#Adding cluster labels to RFM dataset
rfm_data_1$cluster <- as.factor(kmeans_model$cluster)


#visualising the clusters
plot_ly(rfm_data_1, x = ~recency_1, y = ~frequency_1, z = ~monetary_1, color = ~cluster) |>
  add_markers()


# View the customers in each cluster
customer_clusters <- rfm_data_1 |>
  select(customer_id, cluster)

#  View customers in Cluster 1....change 1 to any number from 1 through to 8, to view the customers in another cluster.
cluster_1_customers <- customer_clusters |>
  filter(cluster == 1)

# Count customers in each cluster
cluster_counts <- customer_clusters |>
  group_by(cluster) |>
  summarise(num_customers = n())

print(cluster_counts)

```


```{r}
# Summarize RFM metrics by cluster
cluster_summary <- rfm_data_1 |>
  group_by(cluster) |>
  summarise(
    avg_recency = mean(recency_1),
    avg_frequency = mean(frequency_1),
    avg_monetary = mean(monetary_1),
    num_customers = n()
  )

print(cluster_summary)


# Count unique customer IDs
num_unique_customers <- n_distinct(valid_transactions$customer_id)

# Check if our data is consistent
num_unique_customers == sum(cluster_summary$num_customers) # Our data is consistent
```

# Adding the meaning of the cluster summary
```{r}
interpretations <- data.frame(
  cluster = as.factor(1:8),  # Cluster IDs
  interpretation = c(
    "Low-Value, Low-Engagement Customers: These customers purchase infrequently, spend little, and haven’t bought recently.",
    "Low-Engagement Customers: Slightly more frequent buyers than Cluster 1 but still low spending and engagement.",
    "Moderate Customers: They purchase more frequently and spend significantly more than Clusters 1 & 2. Still, recency is high.",
    "Inactive Customers: Very low frequency and spending, with high recency. These customers have likely churned.",
    "Super Loyalists: Extremely frequent buyers who spend a lot. Highly engaged repeat customers.",
    "Loyal Big Spenders: Frequent purchasers with high monetary values. An important segment for consistent revenue.",
    "High-Value Loyalists: Very frequent buyers with significant spending. Strong revenue contributors.",
    "Low-Value, Moderate-Engagement Customers: Spend slightly more and purchase a bit more frequently than Cluster 1 but still not highly engaged."
  ),
  value_rank = c(7, 6, 4, 8, 1, 3, 2, 5)  # Value ranking: 1 = Most valuable, 8 = Least valuable
)

# Add interpretations to the RFM dataset
cluster_summary <- cluster_summary |>
  left_join(interpretations, by = "cluster")

# View the updated dataset
head(cluster_summary, 10)
```


# Product Performance
```{r}
# Top products by sales revenue
top_products <- valid_transactions_after |>
  group_by(stock_code, description) |>
  summarise(total_quantity = sum(quantity), revenue = sum(total_sales)) |>
  arrange(desc(revenue)) |>
  head(10)

# Plot top-selling products
ggplot(top_products, aes(x = reorder(description, revenue), y = revenue)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 10 Best-Selling Products", x = "Product", y = "Revenue")



# Bottom products by sales revenue
bottom_products <- valid_transactions_after |>
  group_by(stock_code, description) |>
  summarise(total_quantity = sum(quantity), revenue = sum(total_sales)) |>
  arrange(revenue) |>  # Sort by revenue in ascending order
  head(10)  # Select the bottom 10 products

# Plot bottom-selling products
ggplot(bottom_products, aes(x = reorder(description, revenue), y = revenue)) +
  geom_bar(stat = "identity", fill = "red") +
  coord_flip() +
  labs(title = "Bottom 10 Least-Selling Products", x = "Product", y = "Revenue")



```


# Geographic Insights
```{r}
# Sales by Country
# Aggregate sales by country
country_sales <- valid_transactions_after |>
  group_by(country) |>
  summarise(total_sales = sum(total_sales)) |>
  arrange(desc(total_sales))

# Plot sales by country
ggplot(country_sales, aes(x = reorder(country, total_sales), y = total_sales)) +
  geom_bar(stat = "identity", fill = "green") +
  coord_flip() +
  labs(title = "Total Sales by Country", x = "Country", y = "Total Sales")

```
### Test 1 for growth potential
```{r}
# Aggregate sales data by country
country_sales <- valid_transactions_after |>
  group_by(country) |>
  summarise(
    total_sales = sum(total_sales, na.rm = TRUE),  # Total sales revenue
    num_transactions = n(),                       # Number of transactions
    avg_transaction_value = mean(total_sales, na.rm = TRUE)  # Average transaction value
  ) |>
  arrange(desc(total_sales))  # Sort by highest total sales

# Add quarter and year columns to the dataset
valid_transactions_after <- valid_transactions_after |>
  mutate(
    year = year(invoice_date),
    quarter = quarter(invoice_date)
  )

# Aggregate sales data by country and quarter
country_quarterly_sales <- valid_transactions_after |>
  group_by(country, year, quarter) |>
  summarise(
    quarterly_sales = sum(total_sales, na.rm = TRUE)
  ) |>
  arrange(country, year, quarter)

# Calculate sales growth by country
country_growth <- country_quarterly_sales |>
  group_by(country) |>
  mutate(
    sales_growth = (quarterly_sales - lag(quarterly_sales)) / lag(quarterly_sales) * 100  # Growth rate %
  )

# View growth rates
print(country_growth)


# Filter top 10 countries by total sales
top_countries <- country_sales |>
  top_n(10, total_sales)

# Plot top 10 countries by total sales
ggplot(top_countries, aes(x = reorder(country, total_sales), y = total_sales)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 10 Countries by Total Sales", x = "Country", y = "Total Sales")


# Filter countries with significant growth data
growth_countries <- country_growth |>
  filter(!is.na(sales_growth)) |>
  group_by(country) |>
  summarise(avg_growth = mean(sales_growth, na.rm = TRUE)) |>
  arrange(desc(avg_growth))

# Plot countries with highest average growth
ggplot(growth_countries[1:10, ], aes(x = reorder(country, avg_growth), y = avg_growth)) +
  geom_bar(stat = "identity", fill = "green") +
  coord_flip() +
  labs(title = "Top 10 Countries by Growth Potential", x = "Country", y = "Average Quarterly Growth (%)")

```

### Test 2 for growth potential
```{r}
valid_transactions_after <- valid_transactions_after |>
  mutate(
    year = year(invoice_date),
    quarter = quarter(invoice_date)
  )

# Aggregate sales data by country and quarter
country_quarterly_sales <- valid_transactions_after |>
  group_by(country, year, quarter) |>
  summarise(
    quarterly_sales = sum(total_sales, na.rm = TRUE)
  ) |>
  arrange(country, year, quarter)

# View data to confirm quarterly_sales exists
head(country_quarterly_sales)


# Summarize total sales volume by country
country_sales <- valid_transactions_after |>
  group_by(country) |>
  summarise(
    total_sales = sum(total_sales),
    total_quantity = sum(quantity),
    num_customers = n_distinct(customer_id)
  ) |>
  arrange(desc(total_sales))

# Plot total sales by country
top_countries <- country_sales |>
  head(10)

ggplot(top_countries, aes(x = reorder(country, total_sales), y = total_sales)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 10 Countries by Sales Volume",
    x = "Country",
    y = "Total Sales"
  ) +
  theme_minimal()

# Extract year from the invoice date
valid_transactions_after <- valid_transactions_after |>
  mutate(year = format(invoice_date, "%Y"))


#-----------------------------------------------
# # Calculate sales by country and year
# country_growth <- valid_transactions_after |>
#   group_by(country, year) |>
#   summarise(total_sales = sum(total_sales)) |>
#   arrange(country, year)
# 
# # Calculate year-over-year growth rate for each country
# country_growth <- country_growth |>
#   group_by(country) |>
#   mutate(
#     sales_growth = (total_sales - lag(total_sales)) / lag(total_sales) * 100
#   )
#----------------------------------------------------

country_growth <- country_quarterly_sales |>
  group_by(country) |>
  arrange(country, year, quarter) |>  # Sort within each country
  mutate(
    sales_growth = (quarterly_sales - lag(quarterly_sales)) / lag(quarterly_sales) * 100
  )


country_growth <- country_growth |>
  mutate(sales_growth = replace_na(sales_growth, 0))  # Replace NA growth values with 0

high_growth_countries <- country_growth |>
  group_by(country) |>
  summarise(
    total_sales = sum(quarterly_sales, na.rm = TRUE),
    avg_quarterly_growth = mean(sales_growth, na.rm = TRUE)
  ) |>
  arrange(desc(total_sales), desc(avg_quarterly_growth))

# Print the high-growth countries
print(high_growth_countries)

#------------------------------------------------------
# View countries with the highest growth
top_growth_countries <- country_growth |>
  filter(!is.na(sales_growth)) |>
  arrange(desc(sales_growth)) |>
  head(10)

print(top_growth_countries)



ggplot(top_growth_countries, aes(x = reorder(country, sales_growth), y = sales_growth)) +
  geom_bar(stat = "identity", fill = "green") +
  coord_flip() +
  labs(
    title = "Top 10 Countries by Sales Growth",
    x = "Country",
    y = "Sales Growth (%)"
  ) +
  theme_minimal()
```


# Return Analysis - Cancelled Transactions
```{r}
# Product Return Patterns
# Top returned products
product_returns <- cancelled_transactions |>
  group_by(stock_code, description) |>
  summarise(total_returns = abs(sum(quantity)), revenue_lost = abs(sum(total_sales))) |>
  arrange(desc(total_returns)) |>
  head(10)

# Plot top returned products
ggplot(product_returns, aes(x = reorder(description, total_returns), y = total_returns)) +
  geom_bar(stat = "identity", fill = "red") +
  coord_flip() +
  labs(title = "Top 10 Most Returned Products", x = "Product", y = "Total Returns")


#### Custormer Return Behaviour
# Customers with the highest returns
customer_returns <- cancelled_transactions |>
  group_by(customer_id) |>
  summarise(total_returns = abs(sum(quantity)), revenue_lost = abs(sum(total_sales))) |>
  arrange(desc(total_returns)) |>
  head(10)

# Plot customers with the most returns
ggplot(customer_returns, aes(x = reorder(customer_id, total_returns), y = total_returns)) +
  geom_bar(stat = "identity", fill = "orange") +
  coord_flip() +
  labs(title = "Top 10 Customers with Most Returns", x = "Customer ID", y = "Total Returns")


#### Refund rate by Country
# Refund rates by country
country_returns <- cancelled_transactions |>
  group_by(country) |>
  summarise(total_returns = abs(sum(quantity)))

country_sales <- valid_transactions_after |>
  group_by(country) |>
  summarise(total_sales = sum(quantity))

# Merge and calculate refund rate
refund_rate <- country_returns |>
  left_join(country_sales, by = "country") |>
  mutate(return_rate = total_returns / total_sales)

# Plot refund rates
ggplot(refund_rate, aes(x = reorder(country, return_rate), y = return_rate)) +
  geom_bar(stat = "identity", fill = "purple") +
  coord_flip() +
  labs(title = "Refund Rates by Country", x = "Country", y = "Return Rate")
```


############## Outliers Analysis###########
```{r}
summary(outliers_removed)
```

```{r}
ggplot(outliers_removed, aes(y = price)) +
  geom_boxplot(fill = "blue") +
  ggtitle("Price Outliers Boxplot") +
  theme_minimal()


ggplot(outliers_removed, aes(y = quantity)) +
  geom_boxplot(fill = "blue") +
  ggtitle("Quantity Outliers Boxplot") +
  theme_minimal()
```


